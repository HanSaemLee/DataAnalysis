{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 텍스트 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 토큰화(Tokenization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import nltk         # NLTK는 영어 코퍼스를 토큰화하기 위한 도구를 제공\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) 단어 토큰화\r\n",
    "- 토큰의 단위가 단어일 때"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# word_tokenize가 아포스트로피 (Don't와 Jone's)를 처리하는 방법\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "print(word_tokenize(sample))\r\n",
    "# Don't -> Do, n't, Jone's -> Jone, 's"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# wordPunctTokenizer가 아포스트로피 (Don't와 Jone's)를 처리하는 법\r\n",
    "from nltk.tokenize import WordPunctTokenizer\r\n",
    "print(WordPunctTokenizer().tokenize(sample))\r\n",
    "# Don't -> Don, ', t , Jone's -> Jone, ', s"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# text_to_word_sequence가 아포스트로피 (Don't와 Jone's)를 처리하는 법\r\n",
    "# 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거함.\r\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\r\n",
    "print(text_to_word_sequence(sample))\r\n",
    "# Don't와 Jone's의 아포스트로피 보존\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 토큰화에서 주의해야할 사항\r\n",
    "- 구두점이나 특수 문자를 단순 제외해서는 안 된다.\r\n",
    "- 줄임말과 단어 내에 띄어쓰기가 있는 경우"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\r\n",
    "tokenizer = TreebankWordTokenizer()\r\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\r\n",
    "print(tokenizer.tokenize(text))\r\n",
    "\r\n",
    "# home-based는 하나의 토큰으로 취급, doesn't의 경우 does와 n't로 분리"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) 문장 토큰화\r\n",
    "- 토큰의 단위가 문장일 때"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.tokenize import sent_tokenize\r\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\r\n",
    "print(sent_tokenize(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Ph.D.를 문장의 끝으로 인식하지 않음\r\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\r\n",
    "print(sent_tokenize(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 한글 문장 토큰화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# KSS(Korean Sentence splitter)\r\n",
    "!pip install kss"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.1.0.4.tar.gz (42.3 MB)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
      "Building wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-3.1.0.4-py3-none-any.whl size=42336579 sha256=bcf0c6cedd7274455a318c28d534903c98525587e64f031a06faa9be42c4378b\n",
      "  Stored in directory: c:\\users\\hs\\appdata\\local\\pip\\cache\\wheels\\65\\7e\\ae\\6b3a20a8451b2c8154e8f6fc599ffab090cd113c60c65b8f14\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186452 sha256=fa42cecbed658a204ac2e6aa6b23a1523e99f436614bb1c2d8a43b7104214246\n",
      "  Stored in directory: c:\\users\\hs\\appdata\\local\\pip\\cache\\wheels\\71\\4d\\3c\\cada364d4ea0026deee7208dee1e61bcebd20aa2ae5dc154ba\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.4.2 kss-3.1.0.4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import kss\r\n",
    "\r\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\r\n",
    "print(kss.split_sentences(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Korean Sentence Splitter]: Initializing Kss...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) 품사(POS)태깅"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\r\n",
    "print(word_tokenize(text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- NLTK는 영어 코퍼스에 품사태깅 기능을 지원\r\n",
    "- KoNLPy는 한국어 태깅"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 품사 태깅 - PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, \r\n",
    "# IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사\r\n",
    "from nltk.tag import pos_tag\r\n",
    "x=word_tokenize(text)\r\n",
    "pos_tag(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 한글(KoNLPy)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!pip install konlpy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\hs\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from konlpy) (1.20.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\hs\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Okt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 형태소 분석\r\n",
    "from konlpy.tag import Okt  \r\n",
    "okt=Okt()  \r\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 품사 부착\r\n",
    "text = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\r\n",
    "okt.pos(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('열심히', 'Adverb'),\n",
       " ('코딩', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('당신', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('연휴', 'Noun'),\n",
       " ('에는', 'Josa'),\n",
       " ('여행', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('가봐요', 'Verb')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 명사 추출\r\n",
    "okt.nouns(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['코딩', '당신', '연휴', '여행']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 꼬꼬마 형태소 분석기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from konlpy.tag import Kkma  \r\n",
    "kkma=Kkma()  \r\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "kkma.pos(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('열심히', 'MAG'),\n",
       " ('코딩', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('당신', 'NP'),\n",
       " (',', 'SP'),\n",
       " ('연휴', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('는', 'JX'),\n",
       " ('여행', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('가보', 'VV'),\n",
       " ('아요', 'EFN')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 정제 및 정규화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\r\n",
    "import re\r\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\r\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\r\n",
    "print(shortword.sub('', text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) 표제어 추출(Lemmatization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "n = WordNetLemmatizer()\r\n",
    "words=['policy','doing','organization','have','going','love','lives','fly',\r\n",
    "        'dies','watched','has','staring']\r\n",
    "print([n.lemmatize(w) for w in words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'staring']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "n.lemmatize('dies','v'), n.lemmatize('has','v'), n.lemmatize('watched','v')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('die', 'have', 'watch')"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) 어간 추출(Stemming)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "s = PorterStemmer()\r\n",
    "\r\n",
    "# 단순 규칙에 기반하여 이루어진 어간 추출\r\n",
    "\r\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings-- with the single exception of the red corsses and the written notes.\"\r\n",
    "words=word_tokenize(text)\r\n",
    "print(words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'corsses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print([s.stem(w) for w in words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'corss', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "words=['formalize','allowance','electricical']\r\n",
    "print([s.stem(w) for w in words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "words=['policy','doing','organization','have','going','love','lives','fly','dies','watched','has','starting']\r\n",
    "print([s.stem(w) for w in words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 동일한 단어지만 두 스태머는 전혀 다른 결과를 보여준다.\r\n",
    "# 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용\r\n",
    "\r\n",
    "from nltk.stem import LancasterStemmer\r\n",
    "l=LancasterStemmer()\r\n",
    "words=['policy','doing','organization','have','going','love','lives','fly','dies','watched','has','starting']\r\n",
    "print([l.stem(w) for w in words])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 불용어(Stopworods)\r\n",
    "- 문장에서는 자주 등장하지만 실제 의미 분석에는 기여하는 바가 없는 단어"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# NLTK가 정의한 영어 불용어 리스트 갯수\r\n",
    "from nltk.corpus import stopwords\r\n",
    "len(stopwords.words('english'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "stopwords.words('english')[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\r\n",
    "stop_words = set(stopwords.words('english'))\r\n",
    "stop_words.add(\"'s\")        # 추가하고 싶으면 추가하면 됨\r\n",
    "\r\n",
    "word_tokens = word_tokenize(example.lower())\r\n",
    "result = [w for w in word_tokens if w not in stop_words]\r\n",
    "\r\n",
    "print(word_tokens)\r\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['family', 'is', 'not', 'an', 'important', 'thing', '.', 'it', \"'s\", 'everything', '.']\n",
      "['family', 'important', 'thing', '.', 'everything', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울때는 중요한 게 있지.\"\r\n",
    "stop_words = \"아무거나 아무렇게나 이찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\r\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\r\n",
    "stop_words=stop_words.split()\r\n",
    "word_tokens = word_tokenize(example)\r\n",
    "\r\n",
    "result = [w for w in word_tokens if w not in stop_words]\r\n",
    "\r\n",
    "print(word_tokens)\r\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3afaf00aff83f2e8eb9647e300ff648c4cd120434f7e4c9b48fc973ba4a47465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}